{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aab6f5ee-00fd-476d-8677-7fb4b49b0326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "821b8ba1-f0c0-474f-93b4-9c5acd607612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# --- Data Transforms ---\n",
    "mean = (0.4914, 0.4822, 0.4465)\n",
    "std  = (0.2023, 0.1994, 0.2010)\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2daacf48-fb44-4a73-8422-1d54382831ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Defense Modules ---\n",
    "class ThresholdReLU(nn.Module):\n",
    "    def __init__(self, threshold=0.001):\n",
    "        super().__init__()\n",
    "        self.threshold = threshold\n",
    "    def forward(self, x):\n",
    "        return torch.where(x > self.threshold, x, torch.zeros_like(x))\n",
    "\n",
    "def quantize_input(x, levels=16, min_val=-2.5, max_val=2.5):\n",
    "    x = torch.clamp(x, min_val, max_val)\n",
    "    x_norm = (x - min_val) / (max_val - min_val)\n",
    "    x_quant = torch.round(x_norm * (levels - 1)) / (levels - 1)\n",
    "    return x_quant * (max_val - min_val) + min_val\n",
    "\n",
    "# Normalized sparsity loss (L1 norm / num elements)\n",
    "def sparsity_loss(activations, weight=1e-6):\n",
    "    total_elems = sum(act.numel() for act in activations)\n",
    "    l1 = sum(torch.norm(act, 1) for act in activations)\n",
    "    return weight * (l1 / total_elems)\n",
    "\n",
    "# Modified sparsity loss (tanh-approx to encourage nonzero activations)\n",
    "def sparsity_loss_modified(activations, beta=10.0):\n",
    "    total_elems = sum(act.numel() for act in activations)\n",
    "    loss = 0.0\n",
    "    for act in activations:\n",
    "        # Approximate nonzero indicator: tanh(beta * |act|)\n",
    "        loss += torch.sum(1.0 - torch.tanh(beta * torch.abs(act)))\n",
    "    return loss / total_elems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "88fa9fbd-0907-43b6-a4b8-7de9e426405b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model Definition ---\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = ThresholdReLU()\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "    def forward(self, x):\n",
    "        out1 = self.relu(self.bn1(self.conv1(x)))\n",
    "        out2 = self.bn2(self.conv2(out1))\n",
    "        out2 += self.shortcut(x)\n",
    "        out = self.relu(out2)\n",
    "        return out, out1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eff3cab2-311b-467f-980a-7d9e2a826e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet56(nn.Module):\n",
    "    def __init__(self, block=BasicBlock, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.in_channels = 16\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.relu = ThresholdReLU()\n",
    "        self.layer1 = self._make_layer(block, 16, blocks=9, stride=1)\n",
    "        self.layer2 = self._make_layer(block, 32, blocks=9, stride=2)\n",
    "        self.layer3 = self._make_layer(block, 64, blocks=9, stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(64, num_classes)\n",
    "    def _make_layer(self, block, out_channels, blocks, stride):\n",
    "        strides = [stride] + [1]*(blocks-1)\n",
    "        layers = []\n",
    "        for s in strides:\n",
    "            layers.append(block(self.in_channels, out_channels, s))\n",
    "            self.in_channels = out_channels\n",
    "        return nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        activations = []\n",
    "        for block in self.layer1:\n",
    "            out, act = block(out); activations.append(act)\n",
    "        for block in self.layer2:\n",
    "            out, act = block(out); activations.append(act)\n",
    "        for block in self.layer3:\n",
    "            out, act = block(out); activations.append(act)\n",
    "        out = self.avgpool(out)\n",
    "        out = torch.flatten(out, 1)\n",
    "        out = self.fc(out)\n",
    "        return out, activations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "27a5f877-1f9c-42e3-a36b-aa7eefb965c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Training and Evaluation ---\n",
    "def train_one_epoch(model, loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    for imgs, labels in loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        imgs = quantize_input(imgs)\n",
    "        outputs, activations = model(imgs)\n",
    "        loss_main = criterion(outputs, labels)\n",
    "        loss_sparse = sparsity_loss(activations)\n",
    "        loss = loss_main + loss_sparse\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()*imgs.size(0)\n",
    "        _, preds = outputs.max(1)\n",
    "        correct += preds.eq(labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    return total_loss/total, 100*correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a5de0d55-2684-46f4-a701-fc733170124e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            imgs = quantize_input(imgs)\n",
    "            outputs, activations = model(imgs)\n",
    "            loss_main = criterion(outputs, labels)\n",
    "            loss_sparse = sparsity_loss(activations)\n",
    "            loss = loss_main + loss_sparse\n",
    "            total_loss += loss.item()*imgs.size(0)\n",
    "            _, preds = outputs.max(1)\n",
    "            correct += preds.eq(labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return total_loss/total, 100*correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "339fd5f3-0ee0-47b3-873a-55891fd26834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- White-box Sparsity Attack ---\n",
    "def generate_sparsity_adversary(model, x_clean, y_clean, criterion, epsilon=0.2, alpha=0.01, num_iter=50, c=1.0):\n",
    "    model.eval()\n",
    "    x_adv = x_clean.clone().detach().to(device); x_adv.requires_grad=True\n",
    "    for i in range(num_iter):\n",
    "        # Skip quantization here for clearer gradients\n",
    "        outputs, activations = model(x_adv)\n",
    "        loss_ce = criterion(outputs, y_clean)\n",
    "        loss_sp = sparsity_loss_modified(activations)\n",
    "        loss = loss_sp + c * loss_ce\n",
    "        model.zero_grad(); loss.backward()\n",
    "        grad = x_adv.grad.data\n",
    "        x_adv = x_adv - alpha * grad.sign()\n",
    "        x_adv = torch.max(torch.min(x_adv, x_clean+epsilon), x_clean-epsilon)\n",
    "        x_adv = torch.clamp(x_adv, -2.5, 2.5)\n",
    "        x_adv = x_adv.detach(); x_adv.requires_grad=True\n",
    "        if (i+1)%10==0:\n",
    "            print(f\"Iter {(i+1)}/{num_iter}, Loss: {loss.item():.4f}, CE: {loss_ce.item():.4f}, SP: {loss_sp.item():.4f}\")\n",
    "    return x_adv.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4de82498-2fa4-4944-a6a1-b022da763ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Instantiate, Train/Load, and Test Attack ---\n",
    "model = ResNet56().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9615b224-2541-49e3-86ef-9403d1e7de01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 10/50, Loss: 2.8668, CE: 2.1939, SP: 0.6729\n",
      "Iter 20/50, Loss: 2.8424, CE: 2.1818, SP: 0.6607\n",
      "Iter 30/50, Loss: 2.8385, CE: 2.1794, SP: 0.6590\n",
      "Iter 40/50, Loss: 2.8377, CE: 2.1788, SP: 0.6589\n",
      "Iter 50/50, Loss: 2.8373, CE: 2.1785, SP: 0.6588\n",
      "Clean preds: tensor([0, 0, 0, 0, 5, 0, 0, 0, 0, 0], device='cuda:0')\n",
      "Adv preds:  tensor([0, 0, 0, 0, 5, 0, 0, 0, 0, 0], device='cuda:0')\n",
      "Sparsity clean: 0.5080, adv: 0.5110\n"
     ]
    }
   ],
   "source": [
    "# Example: Test on one batch\n",
    "model.eval()\n",
    "for imgs, labels in testloader:\n",
    "    imgs, labels = imgs.to(device), labels.to(device)\n",
    "    imgs_q = quantize_input(imgs)\n",
    "    out_c, acts_c = model(imgs_q); _, p_c = out_c.max(1)\n",
    "    imgs_adv = generate_sparsity_adversary(model, imgs_q, p_c, criterion)\n",
    "    out_a, acts_a = model(imgs_adv); _, p_a = out_a.max(1)\n",
    "    print(\"Clean preds:\", p_c[:10])\n",
    "    print(\"Adv preds: \", p_a[:10])\n",
    "    spar_c = sum((act!=0).float().sum() for act in acts_c)/sum(act.numel() for act in acts_c)\n",
    "    spar_a = sum((act!=0).float().sum() for act in acts_a)/sum(act.numel() for act in acts_a)\n",
    "    print(f\"Sparsity clean: {spar_c:.4f}, adv: {spar_a:.4f}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e966530e-dc8e-48b0-9b3c-1c41aef80352",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
